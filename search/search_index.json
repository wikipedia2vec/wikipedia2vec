{
    "docs": [
        {
            "location": "/",
            "text": "Wikipedia2Vec\n\n\n\n\n\nStar\n\n\nWikipedia2Vec is a tool used for obtaining embeddings (or vector representations) of words and entities (i.e., concepts that have corresponding pages in Wikipedia) from Wikipedia.\nIt is developed and maintained by \nStudio Ousia\n.\n\n\nThis tool enables you to learn embeddings of words and entities simultaneously, and places similar words and entities close to one another in a continuous vector space.\nEmbeddings can be easily trained by a single command with a publicly available Wikipedia dump as input.\n\n\nThis tool implements the \nconventional skip-gram model\n to learn the embeddings of words, and its extension proposed in \nYamada et al. (2016)\n to learn the embeddings of entities.\n\n\nThis tool has been tested on Linux, Windows, and macOS.\n\n\nAn empirical comparison between Wikipedia2Vec and existing embedding tools (i.e., FastText, Gensim, RDF2Vec, and Wiki2vec) is available \nhere\n.\n\n\nThe example code of the neural text classification model built upon Wikipedia2Vec is available \nhere\n.\n\n\nPretrained embeddings for 12 languages (i.e., English, Arabic, Chinese, Dutch, French, German, Italian, Japanese, Polish, Portuguese, Russian, and Spanish) can be downloaded from \nthis page\n.\n\n\nUse Cases\n\uf0c1\n\n\n\n\nEntity linking: \nYamada et al., 2016\n, \nEshel et al., 2017\n, \nChen et al., 2019\n\n\nNamed entity recognition: \nSato et al., 2017\n, \nLara-Clares and Garcia-Serrano, 2019\n\n\nQuestion answering: \nYamada et al., 2017\n\n\nParaphrase detection: \nDuong et al., 2018\n\n\nKnowledge graph completion: \nShah et al., 2019\n\n\nPlot analysis of movies: \nPapalampidi et al., 2019\n\n\nText classification: \nYamada et al., 2019\n\n\nEnhancement of BERT using Wikipedia knowledge: \nPoerner et al., 2019\n\n\n\n\nReferences\n\uf0c1\n\n\nIf you use Wikipedia2Vec in a scientific publication, please cite the following paper:\n\n\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, \nWikipedia2Vec: An Optimized Tool for Learning Embeddings of Words and Entities from Wikipedia\n.\n\n\n@article{yamada2018wikipedia2vec,\n  title={Wikipedia2Vec: An Optimized Tool for Learning Embeddings of Words and Entities from Wikipedia},\n  author={Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu},\n  journal={arXiv preprint 1812.06280},\n  year={2018}\n}\n\n\n\n\nWikipedia2Vec is an official implementation of the embedding model proposed in the following paper:\n\n\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, \nJoint Learning of the Embedding of Words and Entities for Named Entity Disambiguation\n.\n\n\n@inproceedings{yamada2016joint,\n  title={Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation},\n  author={Yamada, Ikuya and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu},\n  booktitle={Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},\n  year={2016},\n  publisher={Association for Computational Linguistics},\n  doi={10.18653/v1/K16-1025},\n  pages={250--259}\n}\n\n\n\n\nThe text classification model implemented in \nthis example\n was proposed in the following paper:\n\n\nIkuya Yamada, Hiroyuki Shindo, \nNeural Attentive Bag-of-Entities Model for Text Classification\n.\n\n\n@article{yamada2019neural,\n  title={Neural Attentive Bag-of-Entities Model for Text Classification},\n  author={Yamada, Ikuya and Shindo, Hiroyuki},\n  booktitle={Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},\n  year={2019},\n  publisher={Association for Computational Linguistics}\n}\n\n\n\n\nLicense\n\uf0c1\n\n\nApache License 2.0",
            "title": "Home"
        },
        {
            "location": "/#use-cases",
            "text": "Entity linking:  Yamada et al., 2016 ,  Eshel et al., 2017 ,  Chen et al., 2019  Named entity recognition:  Sato et al., 2017 ,  Lara-Clares and Garcia-Serrano, 2019  Question answering:  Yamada et al., 2017  Paraphrase detection:  Duong et al., 2018  Knowledge graph completion:  Shah et al., 2019  Plot analysis of movies:  Papalampidi et al., 2019  Text classification:  Yamada et al., 2019  Enhancement of BERT using Wikipedia knowledge:  Poerner et al., 2019",
            "title": "Use Cases"
        },
        {
            "location": "/#references",
            "text": "If you use Wikipedia2Vec in a scientific publication, please cite the following paper:  Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji,  Wikipedia2Vec: An Optimized Tool for Learning Embeddings of Words and Entities from Wikipedia .  @article{yamada2018wikipedia2vec,\n  title={Wikipedia2Vec: An Optimized Tool for Learning Embeddings of Words and Entities from Wikipedia},\n  author={Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu},\n  journal={arXiv preprint 1812.06280},\n  year={2018}\n}  Wikipedia2Vec is an official implementation of the embedding model proposed in the following paper:  Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji,  Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation .  @inproceedings{yamada2016joint,\n  title={Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation},\n  author={Yamada, Ikuya and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu},\n  booktitle={Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},\n  year={2016},\n  publisher={Association for Computational Linguistics},\n  doi={10.18653/v1/K16-1025},\n  pages={250--259}\n}  The text classification model implemented in  this example  was proposed in the following paper:  Ikuya Yamada, Hiroyuki Shindo,  Neural Attentive Bag-of-Entities Model for Text Classification .  @article{yamada2019neural,\n  title={Neural Attentive Bag-of-Entities Model for Text Classification},\n  author={Yamada, Ikuya and Shindo, Hiroyuki},\n  booktitle={Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},\n  year={2019},\n  publisher={Association for Computational Linguistics}\n}",
            "title": "References"
        },
        {
            "location": "/#license",
            "text": "Apache License 2.0",
            "title": "License"
        },
        {
            "location": "/intro/",
            "text": "Introduction\n\uf0c1\n\n\n\n\n\nWikipedia2Vec is a tool for learning embeddings of words and entities from Wikipedia.\nThe learned embeddings map similar words and entities close to one another in a continuous vector space.\n\n\nThis tool learns embeddings of words and entities by iterating over entire Wikipedia pages and jointly optimizing the following three submodels:\n\n\n\n\n\n\nWikipedia link graph model\n, which learns entity embeddings by predicting neighboring entities in Wikipedia's link graph, an undirected graph whose nodes are entities and edges represent links between entities, given each entity in Wikipedia.\nHere, an edge is created between a pair of entities if the page of one entity has a link to that of the other entity or if both pages link to each other.\n\n\n\n\n\n\nWord-based skip-gram model\n, which learns word embeddings by predicting neighboring words given each word in a text contained on a Wikipedia page.\n\n\n\n\n\n\nAnchor context model\n, which aims to place similar words and entities near one another in the vector space, and to create interactions between embeddings of words and those of entities.\nHere, we obtain referent entities and their neighboring words from links contained in a Wikipedia page, and the model learns embeddings by predicting neighboring words given each entity.\n\n\n\n\n\n\nThese three submodels are based on the \nskip-gram model\n, which is a neural network model with a training objective to find embeddings that are useful for predicting context items (i.e., neighboring words or entities) given a target item.\nFor further details, please refer to this paper: \nJoint Learning of the Embedding of Words and Entities for Named Entity Disambiguation\n.\n\n\nOptimized Implementation for Learning Embeddings\n\uf0c1\n\n\nWikipedia2Vec is implemented in Python, and most of its code is converted into C++ using Cython to boost its performance.\nLinear algebraic operations required to learn embeddings are performed by Basic Linear Algebra Subprograms (BLAS).\nWe store the embeddings as a float matrix in a shared memory space and update it in parallel using multiple processes.\n\n\nAutomatic Generation of Entity Links\n\uf0c1\n\n\nOne challenge is that many entity names do not appear as links in Wikipedia.\nThis is because Wikipedia instructs its contributors to \ncreate a link only when the name first occurs on the page\n.\nThis is problematic because Wikipedia2Vec uses links as a source to learn embeddings.\n\n\nTo address this, our tool provides a feature that automatically generates links.\nIt first creates a dictionary that maps each entity name to its possible referent entities.\nThis is done by extracting all names and their referring entities from all links contained in Wikipedia.\nThen, during training, our tool takes all words and phrases from the target page and converts each into a link to an entity, if the entity is referred by a link on the same page, or if there is only one referent entity associated to the name in the dictionary.",
            "title": "Introduction"
        },
        {
            "location": "/intro/#introduction",
            "text": "Wikipedia2Vec is a tool for learning embeddings of words and entities from Wikipedia.\nThe learned embeddings map similar words and entities close to one another in a continuous vector space.  This tool learns embeddings of words and entities by iterating over entire Wikipedia pages and jointly optimizing the following three submodels:    Wikipedia link graph model , which learns entity embeddings by predicting neighboring entities in Wikipedia's link graph, an undirected graph whose nodes are entities and edges represent links between entities, given each entity in Wikipedia.\nHere, an edge is created between a pair of entities if the page of one entity has a link to that of the other entity or if both pages link to each other.    Word-based skip-gram model , which learns word embeddings by predicting neighboring words given each word in a text contained on a Wikipedia page.    Anchor context model , which aims to place similar words and entities near one another in the vector space, and to create interactions between embeddings of words and those of entities.\nHere, we obtain referent entities and their neighboring words from links contained in a Wikipedia page, and the model learns embeddings by predicting neighboring words given each entity.    These three submodels are based on the  skip-gram model , which is a neural network model with a training objective to find embeddings that are useful for predicting context items (i.e., neighboring words or entities) given a target item.\nFor further details, please refer to this paper:  Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation .",
            "title": "Introduction"
        },
        {
            "location": "/intro/#optimized-implementation-for-learning-embeddings",
            "text": "Wikipedia2Vec is implemented in Python, and most of its code is converted into C++ using Cython to boost its performance.\nLinear algebraic operations required to learn embeddings are performed by Basic Linear Algebra Subprograms (BLAS).\nWe store the embeddings as a float matrix in a shared memory space and update it in parallel using multiple processes.",
            "title": "Optimized Implementation for Learning Embeddings"
        },
        {
            "location": "/intro/#automatic-generation-of-entity-links",
            "text": "One challenge is that many entity names do not appear as links in Wikipedia.\nThis is because Wikipedia instructs its contributors to  create a link only when the name first occurs on the page .\nThis is problematic because Wikipedia2Vec uses links as a source to learn embeddings.  To address this, our tool provides a feature that automatically generates links.\nIt first creates a dictionary that maps each entity name to its possible referent entities.\nThis is done by extracting all names and their referring entities from all links contained in Wikipedia.\nThen, during training, our tool takes all words and phrases from the target page and converts each into a link to an entity, if the entity is referred by a link on the same page, or if there is only one referent entity associated to the name in the dictionary.",
            "title": "Automatic Generation of Entity Links"
        },
        {
            "location": "/install/",
            "text": "Installation\n\uf0c1\n\n\n\n\nWikipedia2Vec can be installed from PyPI:\n\n\n% pip install wikipedia2vec\n\n\n\n\nAlternatively, you can install the development version of this software from the GitHub repository:\n\n\n% git clone https://github.com/studio-ousia/wikipedia2vec.git\n% cd wikipedia2vec\n% pip install Cython\n% ./cythonize.sh\n% pip install .\n\n\n\n\nWikipedia2Vec requires the 64-bit version of Python, and can be run on Linux, Windows, and Mac OSX.\nIt currently depends on the following Python libraries: \nclick\n, \njieba\n, \njoblib\n, \nlmdb\n, \nmarisa-trie\n, \nmwparserfromhell\n, \nnumpy\n, \nscipy\n, \nsix\n, and \ntqdm\n.\n\n\nIf you want to train embeddings on your machine, it is highly recommended to install a BLAS library.\nWe recommend using \nOpenBLAS\n or \nIntel Math Kernel Library\n.\nNote that, the BLAS library needs to be recognized properly from SciPy.\nThis can be confirmed by using the following command:\n\n\n% python -c 'import scipy; scipy.show_config()'\n\n\n\n\nTo process Japanese Wikipedia dumps, it is also required to install \nMeCab\n and \nits Python binding\n.\nFurthermore, to use \nICU library\n to split either words or sentences or both, you need to install the \nC/C++ ICU library\n and the \nPyICU\n library.",
            "title": "Installation"
        },
        {
            "location": "/install/#installation",
            "text": "Wikipedia2Vec can be installed from PyPI:  % pip install wikipedia2vec  Alternatively, you can install the development version of this software from the GitHub repository:  % git clone https://github.com/studio-ousia/wikipedia2vec.git\n% cd wikipedia2vec\n% pip install Cython\n% ./cythonize.sh\n% pip install .  Wikipedia2Vec requires the 64-bit version of Python, and can be run on Linux, Windows, and Mac OSX.\nIt currently depends on the following Python libraries:  click ,  jieba ,  joblib ,  lmdb ,  marisa-trie ,  mwparserfromhell ,  numpy ,  scipy ,  six , and  tqdm .  If you want to train embeddings on your machine, it is highly recommended to install a BLAS library.\nWe recommend using  OpenBLAS  or  Intel Math Kernel Library .\nNote that, the BLAS library needs to be recognized properly from SciPy.\nThis can be confirmed by using the following command:  % python -c 'import scipy; scipy.show_config()'  To process Japanese Wikipedia dumps, it is also required to install  MeCab  and  its Python binding .\nFurthermore, to use  ICU library  to split either words or sentences or both, you need to install the  C/C++ ICU library  and the  PyICU  library.",
            "title": "Installation"
        },
        {
            "location": "/commands/",
            "text": "Learning Embeddings\n\uf0c1\n\n\n\n\nFirst, you need to download a source Wikipedia dump file (e.g., enwiki-latest-pages-articles.xml.bz2) from \nWikimedia Downloads\n.\nThe English dump file can be obtained by running the following command.\n\n\n% wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n\n\n\n\nNote that you do not need to decompress the dump file.\n\n\nThen, the embeddings can be trained from a Wikipedia dump using the \ntrain\n command.\n\n\n% wikipedia2vec train DUMP_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_FILE\n: The Wikipedia dump file\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--dim-size\n: The number of dimensions of the embeddings (default: 100)\n\n\n--window\n: The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)\n\n\n--iteration\n: The number of iterations for Wikipedia pages (default: 5)\n\n\n--negative\n: The number of negative samples (default: 5)\n\n\n--lowercase/--no-lowercase\n: Whether to lowercase words (default: True)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible choices are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--sent-detect\n: The sentence detector used to split texts into sentences. Currently, only \nicu\n is the possible value (default: None)\n\n\n--min-word-count\n: A word is ignored if the total frequency of the word is less than this value (default: 10)\n\n\n--min-entity-count\n: An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)\n\n\n--min-paragraph-len\n: A paragraph is ignored if its length is shorter than this value (default: 5)\n\n\n--category/--no-category\n: Whether to include Wikipedia categories in the dictionary (default:False)\n\n\n--disambi/--no-disambi\n: Whether to include disambiguation entities in the dictionary (default:False)\n\n\n--link-graph/--no-link-graph\n: Whether to learn from the Wikipedia link graph (default: True)\n\n\n--entities-per-page\n: For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)\n\n\n--link-mentions\n: Whether to convert entity names into links (default: True)\n\n\n--min-link-prob\n: An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)\n\n\n--min-prior-prob\n: An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)\n\n\n--max-mention-len\n: The maximum number of characters in an entity name (default: 20)\n\n\n--init-alpha\n: The initial learning rate (default: 0.025)\n\n\n--min-alpha\n: The minimum learning rate (default: 0.0001)\n\n\n--sample\n: The parameter that controls the downsampling of frequent words (default: 1e-4)\n\n\n--word-neg-power\n: Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)\n\n\n--entity-neg-power\n: Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nThe \ntrain\n command internally calls the five commands described below (namely, \nbuild-dump-db\n, \nbuild-dictionary\n, \nbuild-link-graph\n, \nbuild-mention-db\n, and \ntrain-embedding\n).\nFurther, the learned model file can be converted to a text file compatible with the format of \nWord2vec\n and \nGloVe\n using the \nsave-text\n command.\n\n\nBuilding Dump Database\n\uf0c1\n\n\nThe \nbuild-dump-db\n command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it.\n\n\n% wikipedia2vec build-dump-db DUMP_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_FILE\n: The Wikipedia dump file\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nBuilding Dictionary\n\uf0c1\n\n\nThe \nbuild-dictionary\n command builds a dictionary of words and entities.\n\n\n% wikipedia2vec build-dictionary DUMP_DB_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild-dump-db\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--lowercase/--no-lowercase\n: Whether to lowercase words (default: True)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible choices are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--min-word-count\n: A word is ignored if the total frequency of the word is less than this value (default: 10)\n\n\n--min-entity-count\n: An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)\n\n\n--min-paragraph-len\n: A paragraph is ignored if its length is shorter than this value (default: 5)\n\n\n--category/--no-category\n: Whether to include Wikipedia categories in the dictionary (default:False)\n\n\n--disambi/--no-disambi\n: Whether to include disambiguation entities in the dictionary (default:False)\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nBuilding Link Graph (Optional)\n\uf0c1\n\n\nThe \nbuild-link-graph\n command generates a sparse matrix representing the link structure between Wikipedia entities.\n\n\n% wikipedia2vec build-link-graph DUMP_DB_FILE DIC_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild-dump-db\n command\n\n\nDIC_FILE\n: The dictionary file generated by the \nbuild-dictionary\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nBuilding Mention DB (Optional)\n\uf0c1\n\n\nThe \nbuild-mention-db\n command builds a database that contains the mappings of entity names (mentions) and their possible referent entities.\n\n\n% wikipedia2vec build-mention-db DUMP_DB_FILE DIC_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild-dump-db\n command\n\n\nDIC_FILE\n: The dictionary file generated by the \nbuild-dictionary\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--min-link-prob\n: An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)\n\n\n--min-prior-prob\n: An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)\n\n\n--max-mention-len\n: The maximum number of characters in an entity name (default: 20)\n\n\n--case-sensitive\n: Whether to detect entity names in a case sensitive manner (default: False)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible choices are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nLearning Embeddings\n\uf0c1\n\n\nThe \ntrain-embedding\n command runs the training of the embeddings.\n\n\n% wikipedia2vec train-embedding DUMP_DB_FILE DIC_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild-dump-db\n command\n\n\nDIC_FILE\n: The dictionary file generated by the \nbuild-dictionary\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--link-graph\n: The link graph file generated using the \nbuild-link-graph\n command\n\n\n--mention-db\n: The mention DB file generated using the \nbuild-mention-db\n command\n\n\n--dim-size\n: The number of dimensions of the embeddings (default: 100)\n\n\n--window\n: The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)\n\n\n--iteration\n: The number of iterations for Wikipedia pages (default: 5)\n\n\n--negative\n: The number of negative samples (default: 5)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible values are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--sent-detect\n: The sentence detector used to split texts into sentences. Currently, only \nicu\n is the possible value (default: None)\n\n\n--entities-per-page\n: For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)\n\n\n--init-alpha\n: The initial learning rate (default: 0.025)\n\n\n--min-alpha\n: The minimum learning rate (default: 0.0001)\n\n\n--sample\n: The parameter that controls the downsampling of frequent words (default: 1e-4)\n\n\n--word-neg-power\n: Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)\n\n\n--entity-neg-power\n: Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nSaving Embeddings in Text Format\n\uf0c1\n\n\nsave-text\n outputs a model in a text format.\n\n\n% wikipedia2vec save-text MODEL_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nMODEL_FILE\n: The model file generated by the \ntrain-embedding\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--out-format\n: The output format. Possible values are \ndefault\n, \nword2vec\n, and \nglove\n. If \nword2vec\n and \nglove\n are specified, the format adopted by \nWord2Vec\n and \nGloVe\n are used, respectively.",
            "title": "Learning Embeddings"
        },
        {
            "location": "/commands/#learning-embeddings",
            "text": "First, you need to download a source Wikipedia dump file (e.g., enwiki-latest-pages-articles.xml.bz2) from  Wikimedia Downloads .\nThe English dump file can be obtained by running the following command.  % wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2  Note that you do not need to decompress the dump file.  Then, the embeddings can be trained from a Wikipedia dump using the  train  command.  % wikipedia2vec train DUMP_FILE OUT_FILE  Arguments:   DUMP_FILE : The Wikipedia dump file  OUT_FILE : The output file   Options:   --dim-size : The number of dimensions of the embeddings (default: 100)  --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)  --iteration : The number of iterations for Wikipedia pages (default: 5)  --negative : The number of negative samples (default: 5)  --lowercase/--no-lowercase : Whether to lowercase words (default: True)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are  regexp ,  icu ,  mecab , and  jieba  --sent-detect : The sentence detector used to split texts into sentences. Currently, only  icu  is the possible value (default: None)  --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10)  --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)  --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5)  --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False)  --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False)  --link-graph/--no-link-graph : Whether to learn from the Wikipedia link graph (default: True)  --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)  --link-mentions : Whether to convert entity names into links (default: True)  --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)  --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)  --max-mention-len : The maximum number of characters in an entity name (default: 20)  --init-alpha : The initial learning rate (default: 0.025)  --min-alpha : The minimum learning rate (default: 0.0001)  --sample : The parameter that controls the downsampling of frequent words (default: 1e-4)  --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)  --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)  --pool-size : The number of worker processes (default: the number of CPUs)   The  train  command internally calls the five commands described below (namely,  build-dump-db ,  build-dictionary ,  build-link-graph ,  build-mention-db , and  train-embedding ).\nFurther, the learned model file can be converted to a text file compatible with the format of  Word2vec  and  GloVe  using the  save-text  command.",
            "title": "Learning Embeddings"
        },
        {
            "location": "/commands/#building-dump-database",
            "text": "The  build-dump-db  command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it.  % wikipedia2vec build-dump-db DUMP_FILE OUT_FILE  Arguments:   DUMP_FILE : The Wikipedia dump file  OUT_FILE : The output file   Options:   --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Dump Database"
        },
        {
            "location": "/commands/#building-dictionary",
            "text": "The  build-dictionary  command builds a dictionary of words and entities.  % wikipedia2vec build-dictionary DUMP_DB_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build-dump-db  command  OUT_FILE : The output file   Options:   --lowercase/--no-lowercase : Whether to lowercase words (default: True)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are  regexp ,  icu ,  mecab , and  jieba  --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10)  --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)  --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5)  --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False)  --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False)  --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Dictionary"
        },
        {
            "location": "/commands/#building-link-graph-optional",
            "text": "The  build-link-graph  command generates a sparse matrix representing the link structure between Wikipedia entities.  % wikipedia2vec build-link-graph DUMP_DB_FILE DIC_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build-dump-db  command  DIC_FILE : The dictionary file generated by the  build-dictionary  command  OUT_FILE : The output file   Options:   --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Link Graph (Optional)"
        },
        {
            "location": "/commands/#building-mention-db-optional",
            "text": "The  build-mention-db  command builds a database that contains the mappings of entity names (mentions) and their possible referent entities.  % wikipedia2vec build-mention-db DUMP_DB_FILE DIC_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build-dump-db  command  DIC_FILE : The dictionary file generated by the  build-dictionary  command  OUT_FILE : The output file   Options:   --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)  --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)  --max-mention-len : The maximum number of characters in an entity name (default: 20)  --case-sensitive : Whether to detect entity names in a case sensitive manner (default: False)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are  regexp ,  icu ,  mecab , and  jieba  --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Mention DB (Optional)"
        },
        {
            "location": "/commands/#learning-embeddings_1",
            "text": "The  train-embedding  command runs the training of the embeddings.  % wikipedia2vec train-embedding DUMP_DB_FILE DIC_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build-dump-db  command  DIC_FILE : The dictionary file generated by the  build-dictionary  command  OUT_FILE : The output file   Options:   --link-graph : The link graph file generated using the  build-link-graph  command  --mention-db : The mention DB file generated using the  build-mention-db  command  --dim-size : The number of dimensions of the embeddings (default: 100)  --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)  --iteration : The number of iterations for Wikipedia pages (default: 5)  --negative : The number of negative samples (default: 5)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible values are  regexp ,  icu ,  mecab , and  jieba  --sent-detect : The sentence detector used to split texts into sentences. Currently, only  icu  is the possible value (default: None)  --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)  --init-alpha : The initial learning rate (default: 0.025)  --min-alpha : The minimum learning rate (default: 0.0001)  --sample : The parameter that controls the downsampling of frequent words (default: 1e-4)  --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)  --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)  --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Learning Embeddings"
        },
        {
            "location": "/commands/#saving-embeddings-in-text-format",
            "text": "save-text  outputs a model in a text format.  % wikipedia2vec save-text MODEL_FILE OUT_FILE  Arguments:   MODEL_FILE : The model file generated by the  train-embedding  command  OUT_FILE : The output file   Options:   --out-format : The output format. Possible values are  default ,  word2vec , and  glove . If  word2vec  and  glove  are specified, the format adopted by  Word2Vec  and  GloVe  are used, respectively.",
            "title": "Saving Embeddings in Text Format"
        },
        {
            "location": "/usage/",
            "text": "API Usage\n\uf0c1\n\n\n\n\nWikipedia2Vec provides functions to access the learned embeddings of words and entities.\n\n\nThe embeddings can be loaded by the \nload()\n method:\n\n\n>>> from wikipedia2vec import Wikipedia2Vec\n>>> wiki2vec = Wikipedia2Vec.load(MODEL_FILE)\n\n\n\n\nThe embeddings of words and those of entities can be obtained using the \nget_word_vector()\n and \nget_entity_vector()\n methods, respectively:\n\n\n>>> wiki2vec.get_word_vector('the')\nmemmap([ 0.01617998, -0.03325786, -0.01397999, -0.00150471,  0.03237337,\n...\n       -0.04226106, -0.19677088, -0.31087297,  0.1071524 , -0.09824426], dtype=float32)\n\n>>> wiki2vec.get_entity_vector('Scarlett Johansson')\nmemmap([-0.19793572,  0.30861306,  0.29620451, -0.01193621,  0.18228433,\n...\n        0.04986198,  0.24383858, -0.01466644,  0.10835337, -0.0697331 ], dtype=float32)\n\n\n\n\nFurthermore, the \nmost_similar()\n method takes an item (i.e., words or entities), and computes most similar items of the item in the vector space based on cosine similarity.\nThe number of items to be returned can be specified as a second argument:\n\n\n>>> wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)\n[(<Word yoda>, 1.0),\n (<Entity Yoda>, 0.84333622),\n (<Word darth>, 0.73328167),\n (<Word kenobi>, 0.7328127),\n (<Word jedi>, 0.7223742)]\n\n>>> wiki2vec.most_similar(wiki2vec.get_entity('Scarlett Johansson'), 5)\n[(<Entity Scarlett Johansson>, 1.0),\n (<Entity Natalie Portman>, 0.75090045),\n (<Entity Eva Mendes>, 0.73651594),\n (<Entity Emma Stone>, 0.72868186),\n (<Entity Cameron Diaz>, 0.72390842)]",
            "title": "API Usage"
        },
        {
            "location": "/usage/#api-usage",
            "text": "Wikipedia2Vec provides functions to access the learned embeddings of words and entities.  The embeddings can be loaded by the  load()  method:  >>> from wikipedia2vec import Wikipedia2Vec\n>>> wiki2vec = Wikipedia2Vec.load(MODEL_FILE)  The embeddings of words and those of entities can be obtained using the  get_word_vector()  and  get_entity_vector()  methods, respectively:  >>> wiki2vec.get_word_vector('the')\nmemmap([ 0.01617998, -0.03325786, -0.01397999, -0.00150471,  0.03237337,\n...\n       -0.04226106, -0.19677088, -0.31087297,  0.1071524 , -0.09824426], dtype=float32)\n\n>>> wiki2vec.get_entity_vector('Scarlett Johansson')\nmemmap([-0.19793572,  0.30861306,  0.29620451, -0.01193621,  0.18228433,\n...\n        0.04986198,  0.24383858, -0.01466644,  0.10835337, -0.0697331 ], dtype=float32)  Furthermore, the  most_similar()  method takes an item (i.e., words or entities), and computes most similar items of the item in the vector space based on cosine similarity.\nThe number of items to be returned can be specified as a second argument:  >>> wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)\n[(<Word yoda>, 1.0),\n (<Entity Yoda>, 0.84333622),\n (<Word darth>, 0.73328167),\n (<Word kenobi>, 0.7328127),\n (<Word jedi>, 0.7223742)]\n\n>>> wiki2vec.most_similar(wiki2vec.get_entity('Scarlett Johansson'), 5)\n[(<Entity Scarlett Johansson>, 1.0),\n (<Entity Natalie Portman>, 0.75090045),\n (<Entity Eva Mendes>, 0.73651594),\n (<Entity Emma Stone>, 0.72868186),\n (<Entity Cameron Diaz>, 0.72390842)]",
            "title": "API Usage"
        },
        {
            "location": "/pretrained/",
            "text": "Pretrained Embeddings\n\uf0c1\n\n\n\n\nWe provide pretrained embeddings for 12 languages in binary and text format.\nThe binary files can be loaded using the \nWikipedia2Vec.load()\n method (see \nAPI Usage\n).\nThe text files are compatible with the text format of \nWord2vec\n.\nTherefore, these files can be loaded using other libraries such as Gensim's \nload_word2vec_format()\n.\nIn the text files, all entities have a prefix \nENTITY/\n to distinguish them from words.\nNote that it is required to decompress the file before using it.\n\n\nEnglish\n\uf0c1\n\n\n\n\nenwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n  \n500d (bin)\n\n  \n500d (txt)\n\n\nenwiki_20180420_nolg\n (window=5, iteration=10, negative=15, no link graph):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n  \n500d (bin)\n\n  \n500d (txt)\n\n\nenwiki_20180420_win10\n (window=10, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n  \n500d (bin)\n\n  \n500d (txt)\n\n\n\n\nArabic\n\uf0c1\n\n\n\n\narwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nChinese\n\uf0c1\n\n\n\n\nzhwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nDutch\n\uf0c1\n\n\n\n\nnlwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nFrench\n\uf0c1\n\n\n\n\nfrwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nGerman\n\uf0c1\n\n\n\n\ndewiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nItalian\n\uf0c1\n\n\n\n\nitwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nJapanese\n\uf0c1\n\n\n\n\njawiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nPolish\n\uf0c1\n\n\n\n\nplwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nPortuguese\n\uf0c1\n\n\n\n\nptwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nRussian\n\uf0c1\n\n\n\n\nruwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nSpanish\n\uf0c1\n\n\n\n\neswiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)",
            "title": "Pretrained Embeddings"
        },
        {
            "location": "/pretrained/#pretrained-embeddings",
            "text": "We provide pretrained embeddings for 12 languages in binary and text format.\nThe binary files can be loaded using the  Wikipedia2Vec.load()  method (see  API Usage ).\nThe text files are compatible with the text format of  Word2vec .\nTherefore, these files can be loaded using other libraries such as Gensim's  load_word2vec_format() .\nIn the text files, all entities have a prefix  ENTITY/  to distinguish them from words.\nNote that it is required to decompress the file before using it.",
            "title": "Pretrained Embeddings"
        },
        {
            "location": "/pretrained/#english",
            "text": "enwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt) \n   500d (bin) \n   500d (txt)  enwiki_20180420_nolg  (window=5, iteration=10, negative=15, no link graph):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt) \n   500d (bin) \n   500d (txt)  enwiki_20180420_win10  (window=10, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt) \n   500d (bin) \n   500d (txt)",
            "title": "English"
        },
        {
            "location": "/pretrained/#arabic",
            "text": "arwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Arabic"
        },
        {
            "location": "/pretrained/#chinese",
            "text": "zhwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Chinese"
        },
        {
            "location": "/pretrained/#dutch",
            "text": "nlwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Dutch"
        },
        {
            "location": "/pretrained/#french",
            "text": "frwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "French"
        },
        {
            "location": "/pretrained/#german",
            "text": "dewiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "German"
        },
        {
            "location": "/pretrained/#italian",
            "text": "itwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Italian"
        },
        {
            "location": "/pretrained/#japanese",
            "text": "jawiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Japanese"
        },
        {
            "location": "/pretrained/#polish",
            "text": "plwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Polish"
        },
        {
            "location": "/pretrained/#portuguese",
            "text": "ptwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Portuguese"
        },
        {
            "location": "/pretrained/#russian",
            "text": "ruwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Russian"
        },
        {
            "location": "/pretrained/#spanish",
            "text": "eswiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Spanish"
        }
    ]
}